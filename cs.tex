\documentclass{subfile}

\begin{document}
	\section[Cauchy-Schwarz]{Cauchy-Schwarz Inequality and Improvements}\label{sec:cs}
	\index{Cauchy-Schwarz}\textit{Cauchy-Schwarz} inequality also known as \textit{Cauchy-Bunyakovsky-Schwarz} inequality is among the most important results for solving problems.
		\begin{theorem}[\itshape Cauchy-Bunyakovsky-Schwarz inequality]\label{thm:cs}
			Let $n$ be a positive integer, $a_1,\ldots,a_n$ and $b_1,\ldots,b_n$ be real numbers. Then
				\begin{align}
					(a_{1}^{2}+\ldots+a_{n}^{2})(b_{1}^{2}+\ldots+b_{n}^{2})
						& \geq(a_{1}b_{1}+\ldots+a_{n}b_{n})^2\label{ineq:cs}\\
					\left(\sum_{i=1}^{n}a_{i}^{2}\right)\left(\sum_{i=1}^nb_i^2\right)
						& \geq\left(\sum_{i=1}^na_ib_i\right)^2\nonumber
				\end{align}
			and equality holds if and only if $\frac{a_1}{b_1}=\cdots=\frac{a_n}{b_n}$.
		\end{theorem}
	The reason why it is also called \index{Cauchy-Bunyakovsky-Schwarz}Cauchy-Bunyakovsky-Schwarz inequality is that the analog of this inequality for integrals
		\begin{align}
			\left(\int\limits_{a}^{b}f(x)g(x)dx\right)^2
				& \leq\left(\int\limits_{a}^{b}f^2(x)dx\right)\cdot\left(\int\limits_{a}^{b}g^2(x)dx\right)\label{ineq:cbs}
		\end{align}
	appeared in the \textit{M\'{e}moire} \textcite[Page $4$]{bunyakovsky_1846} for the first time. This \textit{M\'{e}moire} was published by the Imperial Academy of Sciences of St. Petersburg in \textcite{bouniakowsky_1859}. \textcite[Page $10$]{steele_2010} states that \textit{Bunyakovsky (1804–1889) had studied in Paris with Cauchy, and he was quite familiar with Cauchy’s work on inequalities; so much so that by the time he came to write his M\'{e}moire, Bunyakovsky was content to refer to the classical form of Cauchy’ inequality for finite sums simply as well-known.}

	This theorem has many proofs and even more applications. We will see later in \autoref{sec:engel} how powerful just a special case of this inequality can be. Let us start with a proof that is probably the most elegant one. Before we show proofs, we will introduce a notation for shortening the \nameref{thm:cs}. Let two vectors (think of a vector as an ordered list of numbers) be $\mathbf{a}=(a_1,\ldots,a_n)$ and $\mathbf{b}=(b_1,\ldots,b_n)$. The numbers $a_{1},\ldots,a_{n}$ inside the vector $\mathbf{a}$ can be called \textit{elements} of $\mathbf{a}$. So, $\mathbf{a}$ has $n$ elements and so does $\mathbf{b}$. Denote by $\langle \mathbf{a},\mathbf{b}\rangle$ the \index{inner product}\textit{inner product} of $\mathbf{a}$ and $\mathbf{b}$ defined as
		\begin{align*}
			\langle \mathbf{a},\mathbf{b}\rangle
				& = a_1b_1+\ldots+a_nb_n
		\end{align*}
	Then the theorem can be stated as
		\begin{align*}
			\langle \mathbf{a},\mathbf{b}\rangle^2
				& \leq\langle\mathbf{a},\mathbf{a}\rangle\cdot\langle\mathbf{b},\mathbf{b}\rangle
		\end{align*}
	The inner product has some interesting properties. Note that the mother of inequality in \autoref{ineq:extendedmother} can be  stated as
		\begin{align*}
			\langle\mathbf{x},\mathbf{x}\rangle
				& \geq0
		\end{align*}
	for any $\mathbf{x}=(x_1,\ldots,x_n)$ where $x_i$ is a real number. Similarly, define $\mathbf{y}=(y_1,\ldots,y_n)$. Then we have the following.
		\begin{enumerate}[\itshape (i)]\label{list:innerprops}
			\item $\langle\mathbf{x},\mathbf{x}\rangle=0$ if and only if $\mathbf{x}=\mathbf{0}$ i.e. $\mathbf{x}=(0,\ldots,0)$.
			\item For any real number $\alpha$, $\langle\alpha\mathbf{x},\mathbf{y}\rangle=\alpha\langle\mathbf{x},\mathbf{y}\rangle$.
			\item $\langle\mathbf{x},\mathbf{y}\rangle=\langle\mathbf{y},\mathbf{x}\rangle$.
			\item For $\mathbf{z}=(z_1,\ldots,z_n)$, $\langle\mathbf{x},\mathbf{y}+\mathbf{z}\rangle=\langle\mathbf{x},\mathbf{y}\rangle+\langle\mathbf{x},\mathbf{z}\rangle$.
			\item $\mathbf{x}+\mathbf{y}=(x_{1}+y_{1},\ldots,x_{n}+y_{n})$.
		\end{enumerate}
	For convenience, we can also define $\mathbf{x}^{r}$ as
		\begin{align*}
			\mathbf{x}^{r}
				& = \left(x_{1}^{r},\ldots,x_{n}^{r}\right)
		\end{align*}
	for a real number $r$. Let us generalize this notation further. Let $m$ be a positive integer. Then for $\mathbf{x}=(x_1,\ldots,x_n)$, define
		\begin{align*}
			\| \mathbf{x}\|_{m}
				& = \langle\underbrace{\mathbf{x},\ldots,\mathbf{x}}_{m\mbox{ times}}\rangle^{\frac{1}{m}}\\
				& = \sqrt[m]{x_1^m+\ldots+x_n^m}
		\end{align*}
	For $m=2$, we will simply write $\| \mathbf{x}\| $ instead of $\| \mathbf{x}\| _2$ just like we write $\sqrt{x}$ instead of $\sqrt[2]{x}$. So we can write \nameref{thm:cs} in a more compact form.
		\begin{align*}
			\| \mathbf{a}\| \cdot\| \mathbf{b}\|
				& \geq\langle\mathbf{a},\mathbf{b}\rangle
		\end{align*}
	This notion can be generalized even further for arbitrary real numbers. For a real number $p$, define
		\begin{align*}
			\| \mathbf{x}\|_{p}
				& = \sqrt[p]{x_{1}^{p}+\ldots+x_{n}^{p}}
		\end{align*}
	This is often called the $L_{p}$ \index{norm of vector}\textit{norm} for the vector $\mathbf{x}$. Furthermore, we can define the addition or subtraction between two vectors as
		\begin{align*}
			\mathbf{a}\pm \mathbf{b}
				& = (a_{1}\pm b_{1},\ldots,a_{n}\pm b_{n})\\
			m\mathbf{a}
				& = (ma_{1},\ldots,ma_{n})
		\end{align*}
	These notations will help us shorten some long inequalities later. We will see the benefit of these notations shortly in \nameref{thm:holder} and \nameref{thm:mink}. Let us see the proofs now.
		\begin{proof}[\itshape Proof by vector]
			Consider two vectors $\mathbf{a}=(a_1,\ldots,a_n),\mathbf{b}=(b_1,\ldots,b_n)$ and their modulus
				\begin{align*}
					\| \mathbf{a}\|
						& = \sqrt{a_1^2+\ldots+a_n^2}\\
					\| \mathbf{b}\|
						& = \sqrt{b_1^2+\ldots+b_n^2}
				\end{align*}
			If $\theta$ is the minimum angle between them, from the rule of \textit{dot product} we get
				\begin{align*}
					\mathbf{a}\cdot\mathbf{b}
						& = a_1b_1+\ldots+a_nb_n\\
					\| \mathbf{a}\| \cdot\| \mathbf{b}\| \cdot\cos\theta
						& = a_1b_1+\ldots+a_nb_n\\
					\cos{\theta}
						& = \dfrac{a_1b_1+\ldots+a_nb_n}{\sqrt{a_1^2+\ldots+a_n^2}\sqrt{b_1^2+\ldots+b_n^2}}
				\end{align*}
			Since $-1\leq\cos\theta\leq1$, after squaring we get
				\begin{align*}
					\cos^2\theta
						& \leq1\\
					\left(\dfrac{a_1b_1+\ldots+a_nb_n}{\sqrt{a_1^2+\ldots+a_n^2}\sqrt{b_1^2+\ldots+b_n^2}}\right)^2
						& \leq1\\
					\iff (a_1^2+\ldots+a_n^2)(b_1^2+\ldots+b_n^2)
						& \geq(a_1b_1+\ldots+a_nb_n)^2
				\end{align*}
			Equality occurs if and only $\cos\theta=1$ or when $\mathbf{a}$ and $\mathbf{b}$ are parallel. In other words, when we have
				\begin{align*}
					\dfrac{a_1}{b_1}& = \ldots=\dfrac{a_n}{b_n}
				\end{align*}
		\end{proof}

		\begin{proof}[\itshape Classical proof by Schwarz]
			Consider the quadratic polynomial
				\begin{align*}
					P(x)
						& = \sum_{i=1}^n(a_ix-b_i)^2\\
						& = x^2\sum_{i=1}^na_i^2-2x\sum_{i=1}^na_ib_i+\sum_{i=1}^nb_i^2\\
						& = Ax^2-Bx+C
				\end{align*}
			where $A=\sum_{i=1}^na_i^2,B=2\sum_{i=1}^na_ib_i,C=\sum_{i=1}^nb_i^2$. Setting $x_i\to a_ix-b_i$ in \ref{ineq:extendedmother}, we see that $P(x)\geq0$. Then the discriminant of $P$ must be $\leq0$.
				\begin{align*}
					B^2-4AC
						& \leq0\\
					4(a_1b_1+\ldots+a_nb_n)^2-4(a_1^2+\ldots+a_n^2)(b_1^2+\ldots+b_n^2)
						& \leq0\\
					\iff (a_1^2+\ldots+a_n^2)(b_1^2+\ldots+b_n^2)
						& \geq(a_1b_1+\ldots+a_nb_n)^2
				\end{align*}
			Equality occurs if and only if $P(x)=0$ for some $x$. Then
				\begin{align*}
					x
						& = \dfrac{b_1}{a_1}=\cdots=\dfrac{b_n}{a_n}
				\end{align*}
		\end{proof}

		\begin{proof}[\itshape Proof by arithmetic-geometric mean]
			We use the notations from the proof above. If $A=0$ or $B=0$, then the inequality is an identity. So, assume that $A,B\neq0$. Note the following.
				\begin{align*}
					\dfrac{a_1^2}{A}+\dfrac{b_1^2}{B}
						& \geq2\dfrac{a_1b_1}{\sqrt{AB}}\\
					\vdots\\
					\dfrac{a_n^2}{A}+\dfrac{b_n^2}{B}
						& \geq2\dfrac{a_nb_n}{\sqrt{AB}}
				\end{align*}
			Summing them together,
				\begin{align*}
					\dfrac{a_1^2}{A}+\dfrac{b_1^2}{B}+\ldots+\dfrac{a_n^2}{A}+\dfrac{b_1^2}{B}
						& \geq2\dfrac{a_1b_1+\ldots+a_nb_n}{\sqrt{AB}}\\
					\dfrac{a_1^2+\ldots+a_n^2}{A}+\dfrac{b_1^2+\ldots+b_n^2}{B}
						& \geq2\dfrac{a_1b_1+\ldots+a_nb_n}{\sqrt{AB}}\\
					1+1
						& \geq2\dfrac{a_1b_1+\ldots+a_nb_n}{\sqrt{AB}}\\
					AB
						& \geq\left(a_1b_1+\ldots+a_nb_n\right)^2
				\end{align*}
			Again, equality holds if and only if $\frac{a_i}{A}=\frac{b_i}{B}$ or $\frac{a_i}{b_i}=\frac{A}{B}=c$, a constant for all $i$.
		\end{proof}

		\begin{proof}[\itshape Proof using the mother of inequality]
			Let $x_i=\frac{a_i}{A},y_i=\frac{b_i}{B}$ for $1\leq i\leq n$. We have
				\begin{align*}
					\sum_{i=1}^nx_i^2
						& = \sum_{i=1}^ny_i^2=1
				\end{align*}
			Rewrite the inequality as
				\begin{align*}
					x_1y_1+\ldots+x_ny_n
						& \leq1\\
					\iff2(x_1y_1+\ldots+x_ny_n)
						& \leq2\\
					\iff2(x_1y_1+\ldots+x_ny_n)
						& \leq x_1^2+\ldots+x_n^2+y_1^2+\ldots+y_n^2\\
					\iff(x_i-y_i)^2+\ldots+(x_n-y_n)^2
						& \geq0
				\end{align*}
			This is obviously true.
		\end{proof}

		\begin{proof}[\itshape Proof using sequences]
			Define the sequence $(S_n)$ as
				\begin{align*}
					D_n
						& = a_1b_1+\ldots+a_nb_n\\
					A_n
						& = a_1^2+\ldots+a_n^2\\
					B_n
						& = b_1^2+\ldots+b_n^2\\
					S_n
						& = D_n^2-A_nB_n
				\end{align*}
			We want to show that $S_n\leq0$. See the following.
				\begin{align*}
					D_{n+1}^2-D_n^2
						& = (D_n+a_{n+1}b_{n+1})^2-D_n^2\\
						& = 2a_{n+1}b_{n+1}D_n+a_{n+1}^2b_{n+1}^2\\
					A_{n+1}B_{n+1}-A_nB_n
						& = (A_n+a_{n+1}^2)(B_n+b_{n+1}^2)-A_nB_n\\
						& = a_{n+1}^2B_n+b_{n+1}^2A_n+a_{n+1}^2b_{n+1}^2\\
					S_{n+1}-S_n
						& = D_{n+1}^2-A_{n+1}B_{n+1}-D_n^2+A_nB_n\\
						& = 2a_{n+1}b_{n+1}D_n+a_{n+1}^2b_{n+1}^2\\
						& -a_{n+1}^2B_n-b_{n+1}^2A_n-a_{n+1}^2b_{n+1}^2\\
						& = 2a_{n+1}b_{n+1}D_n-a_{n+1}^2B_n-b_{n+1}^2A_n\\
						& = 2a_{n+1}b_{n+1}(D_{n-1}+a_nb_n)-a_{n+1}^2(B_{n-1}\\
						& +b_{n+1}^2)-b_{n+1}^2(A_{n-1}+a_{n+1}^2)\\
						& = 2a_{n+1}b_{n+1}D_{n-1}+2a_{n+1}b_{n+1}a_nb_n-a_{n+1}^2B_{n-1}\\
						& -a_{n+1}^2b_{n}^2-b_{n+1}^2A_{n-1}-a_{n+1}^2b_{n}^2\\
						& = 2a_{n+1}b_{n+1}D_{n-1}-a_{n+1}^2B_{n-1}\\
						& -b_{n+1}^2A_{n-1}-(a_{n+1}b_n-a_{n}b_{n+1})^2\\
						& \vdots\\
						& = -\left((a_{n+1}b_n-a_nb_{n+1})^2+\ldots+(a_{n+1}b_1-a_1b_{n+1})^2\right)
				\end{align*}
			Clearly, $S_{n+1}-S_n\leq0$, so
				\begin{align*}
					S_{n+1}
						& \leq S_n\leq\cdots S_1=0
				\end{align*}
			This proves the inequality.
		\end{proof}
	The fact about $S_{n+1}-S_n$ also proves the \index{Lagrange Identity}\textit{Lagrange Identity}.
		\begin{align*}
			\left(\sum_{i=1}^na_i^2\right)\left(\sum_{i=1}^nb_i^2\right)-\left(\sum_{i=1}^na_ib_i\right)^2
				& = \sum_{1\leq i< j\leq n}(a_ib_j-a_jb_i)^2
		\end{align*}
	This equation immediately proves the theorem but it is not quite obvious how the identity follows unless someone knows it beforehand. For this reason, some people also call this the Lagrange inequality but we will stick with Cauchy-Schwarz. % Do we show CALLEBAUT1965491 result here? What about Gram's inequality on Gram's matrix of vectors?

	\index{Gram's inequality}\textit{Gram's inequality} is a very nice generalization of \nameref{thm:cs}.
		\begin{theorem}[\itshape Gram's inequality]
			Let $\mathbf{x}_1=(x_{11},\ldots,x_{1n}),\ldots,\mathbf{x}_n=(x_{n1},\ldots,x_{nn})$ be vectors with $n$ elements. Then
				\begin{align*}
					\begin{vmatrix}
						\langle \mathbf{x}_{1},\mathbf{x}_{1}\rangle & \ldots & \langle \mathbf{x}_{1},\mathbf{x}_{n}\rangle\\
						& \ldots & \\
						\langle \mathbf{x}_{n},\mathbf{x}_{1}\rangle & \ldots & \langle \mathbf{x}_{n},\mathbf{x}_{n}\rangle
					\end{vmatrix}
						& \geq0
				\end{align*}
			Equality occurs if and only if there are real numbers $a_1,\ldots,a_n$ such that all of them are not zero at the same time and
				\begin{align*}
					a_{1}\mathbf{x}_{1}+\ldots+a_{n}\mathbf{x}_{n}
						& = \mathbf{0}
				\end{align*}
			where $\mathbf{0}$ is the zero vector with every element $0$.
		\end{theorem}
	This result is very intuitive to say the least. Note that we get \nameref{thm:cs} for $n=2$ since
		\begin{align*}
			\begin{vmatrix}
				\langle\mathbf{x},\mathbf{x}\rangle & \langle\mathbf{x},\mathbf{y}\rangle\\
				\langle\mathbf{y},\mathbf{x}\rangle & \langle\mathbf{y},\mathbf{y}\rangle
			\end{vmatrix}
				& \geq0\\
			\iff\langle\mathbf{x},\mathbf{x}\rangle\cdot\langle\mathbf{y},\mathbf{y}\rangle-\langle\mathbf{x},\mathbf{y}\rangle\cdot\langle\mathbf{y},\mathbf{x}\rangle
				& \geq0
		\end{align*}
	implies \nameref{thm:cs} due to the fact $\langle\mathbf{x},\mathbf{y}\rangle=\langle\mathbf{y},\mathbf{x}\rangle$. The condition of equality in Gram's inequality is \textit{linear dependence}. Let $\mathbf{x}_{1},\ldots,\mathbf{x}_{n}$ be some vectors with $n$ elements. Then they are \index{linearly dependent}\textit{linearly dependent} if and only if there are real numbers $a_{1},\ldots,a_{n}$ not all zero such that
		\begin{align*}
			a_{1}\mathbf{x}_{1}+\ldots+a_{n}\mathbf{x}_{n}
				& = \mathbf{0}
		\end{align*}
	If such $a_{1},\ldots,a_{n}$ do not exist, that is the condition is satisfied only when $a_{1}=\ldots=a_{n}=0$, then $\mathbf{x}_{1},\ldots,\mathbf{x}_{n}$ are \index{linearly independent}\textit{linearly independent}.
\end{document}